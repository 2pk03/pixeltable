{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dbf9c4",
   "metadata": {},
   "source": [
    "# Object Detection in Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dd62f",
   "metadata": {},
   "source": [
    "In this tutorial we'll demonstrate how to use Pixeltable to do frame-by-frame object detection, made simple through Pixeltable's video-related functionality:\n",
    "* automatic frame extraction\n",
    "* running complex functions against frames (in this case, an object detection model)\n",
    "* reassembling frames back into videos\n",
    "\n",
    "We'll be working with a single video file (from Pixeltable's test data directory). Let's download that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "download_url = 'https://raw.github.com/mkornacker/pixeltable/master/docs/source/data/bangkok.mp4'\n",
    "filename, _ = urllib.request.urlretrieve(download_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f7dfa",
   "metadata": {},
   "source": [
    "Let's also switch to using the full window width, which will make looking at videos later easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2852cc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29054d06",
   "metadata": {},
   "source": [
    "## Creating a tutorial database and table\n",
    "\n",
    "In Pixeltable, all data resides in tables, which in turn are assigned to databases.\n",
    "\n",
    "Let's start by creating a client and a `tutorial` database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da120aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pt\n",
    "\n",
    "cl = pt.Client()\n",
    "cl.drop_db('tutorial', ignore_errors=True, force=True)\n",
    "db = cl.create_db('tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213cee6",
   "metadata": {},
   "source": [
    "The table we're going to create to hold our videos will have three columns to begin with: the original video, the frame and a frame index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afd8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    pt.Column('video', pt.VideoType()),\n",
    "    pt.Column('frame', pt.ImageType()),\n",
    "    pt.Column('frame_idx', pt.IntType()),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c4fca",
   "metadata": {},
   "source": [
    "When creating the table, we supply parameters needed for automatic frame extraction during `insert_rows()`/`insert_pandas()` calls:\n",
    "- The `extract_frames_from` argument is the name of the column of type `video` from which to extract frames.\n",
    "- During an `insert_rows()` call, each input row, corresponding to one video, is expanded into one row per frame (subject to the frame rate requested in the `extracted_fps` keyword argument; `0` indicates the full frame rate).\n",
    "- The columns `frame` and `frame_idx` are populated with the frame file path and frame sequence number, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd875c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = db.create_table(\n",
    "    'video_data', cols,\n",
    "    extract_frames_from='video', extracted_frame_col='frame', extracted_frame_idx_col='frame_idx',\n",
    "    extracted_fps=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c5d16",
   "metadata": {},
   "source": [
    "We now insert a single row containing the name of the video file we just downloaded, which is expanded into 462 frames/rows in the `video_data` table.\n",
    "\n",
    "In general, `insert_rows()` takes as its first argument a list of rows, each of which is a list of column values (and in this case, we only need to supply data for the `video` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a12da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.insert_rows([[filename]], columns=['video'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0de07",
   "metadata": {},
   "source": [
    "We loaded a video that shows a busy intersection in Bangkok. Let's look at the first frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[t.frame_idx == 100][t.frame, t.frame.width, t.frame.height].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3efad",
   "metadata": {},
   "source": [
    "When we created the `video_data` table with automatic frame extraction, Pixeltable does not physically store the frames. Instead, Pixeltable re-extracts the frames on retrieval using the frame index, which can be done very efficiently and avoids any storage overhead (which would be very substantial for video frames)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755edf69",
   "metadata": {},
   "source": [
    "## Object Detection as a user-defined function\n",
    "\n",
    "User-defined functions let you customize Pixeltable's functionality for your own data.\n",
    "\n",
    "In this example, we're going use a `torchvision` object detection model (Faster R-CNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=\"DEFAULT\")\n",
    "_ = model.eval()  # switch to inference mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484dcda",
   "metadata": {},
   "source": [
    "Our function converts the image to PyTorch format and obtains a prediction from the model, which is a list of dictionaries with fields `boxes`, `labels`, and `scores` (one per input image). The fields themselves are PyTorch tensors, and we convert them to standard Python lists (so they become JSON-serializable data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pt.function(return_type=pt.JsonType(), param_types=[pt.ImageType()])\n",
    "def detect(img):\n",
    "    t = transforms.ToTensor()(img).cuda()\n",
    "    t = transforms.ConvertImageDtype(torch.float)(t)\n",
    "    result = model([t])[0]\n",
    "    return {\n",
    "        'boxes': result['boxes'].tolist(), 'labels': result['labels'].tolist(), 'scores': result['scores'].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0157b86",
   "metadata": {},
   "source": [
    "We can then use `detect()` in the Pixeltable index operator using standard Python function call syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ab9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[t.frame_idx == 0][t.frame, detect(t.frame)].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f07d2e",
   "metadata": {},
   "source": [
    "This works as expected, and we now add the detections as a computed column `detections` to the table.\n",
    "\n",
    "Running model inference is generally an expensive operation; adding it as a computed column makes sure it only runs once, at the time the row is inserted. After that, the result is available as part of the stored table data.\n",
    "\n",
    "Note that for computed columns of any type other than `image`, the computed values are **always** stored (ie, `stored=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dab11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add_column(pt.Column('detections', computed_with=detect(t.frame)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555b0e8",
   "metadata": {},
   "source": [
    "We can create a simple function `draw_boxes()` to visualize detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageDraw\n",
    "\n",
    "@pt.function(return_type=pt.ImageType(), param_types=[pt.ImageType(), pt.JsonType()])\n",
    "def draw_boxes(img, boxes):\n",
    "    result = img.copy()\n",
    "    d = PIL.ImageDraw.Draw(result)\n",
    "    for box in boxes:\n",
    "        d.rectangle(box, width=3)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a429c",
   "metadata": {},
   "source": [
    "This function takes two arguments:\n",
    "- `img` has type `image` and receives an instance of `PIL.Image.Image`\n",
    "- `boxes` has type `json` and receives a JSON-serializable structure, in this case a list of 4-element lists of floats\n",
    "\n",
    "When we \"call\" this function, we need to pass in the frame and the bounding boxes identified in that frame. The latter can be selected with the JSON path expression `t.detections.boxes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbd1f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t[t.frame_idx == 0][t.frame, draw_boxes(t.frame, t.detections.boxes)].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6943594",
   "metadata": {},
   "source": [
    "Looking at individual frames gives us some idea of how well our detection algorithm works, but it would be more instructive to turn the visualization output back into a video.\n",
    "\n",
    "We do that with the built-in function `make_video()`, which is an aggregation function that takes a frame index (actually: any expression that can be used to order the frames; a timestamp would also work) and an image, and then assembles the sequence of images into a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce224393",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[pt.make_video(t.frame_idx, draw_boxes(t.frame, t.detections.boxes))].group_by(t.video).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77731554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
