{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "import json\n",
    "import urllib.request\n",
    "import tempfile\n",
    "import tqdm\n",
    "\n",
    "import torch, torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d93d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pt\n",
    "import pixeltable.functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dcd7b",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Creating databases and tables and inserting data](#Creating-databases-and-tables-and-inserting-data)\n",
    "    - [Creating a table](#Creating-a-table)\n",
    "    - [Inserting data](#Inserting-data)\n",
    "    - [Versioning in Pixeltable](#Versioning-in-Pixeltable)\n",
    "    - [Data persistence](#Data-persistence)\n",
    "- [Retrieving data](#Retrieving-data)\n",
    "    - [Filtering rows](#Filtering-rows)\n",
    "    - [Selecting output](#Selecting-output)\n",
    "        - [Operations on JSON data](#Operations-on-JSON-data)<br>\n",
    "        - [Operations on image data](#Operations-on-image-data)<br>\n",
    "        - [Image similarity search](#Image-similarity-search)<br>\n",
    "- [User-defined functions](#User-defined-functions)<br>\n",
    "    - [Stored functions](#Stored-functions)<br>\n",
    "    - [Computed columns](#Computed-columns)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29054d06",
   "metadata": {},
   "source": [
    "# Creating databases and tables and inserting data\n",
    "\n",
    "In Pixeltable, all data resides in tables, which in turn are assigned to databases.\n",
    "\n",
    "Let's start by creating a client and a `tutorial` database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da120aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pt.Client()\n",
    "cl.drop_db('tutorial', force=True)\n",
    "db = cl.create_db('tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9347725",
   "metadata": {},
   "source": [
    "In this tutorial we're going to be a working with a subset of the COCO dataset (10 samples each for the train, test, and validation splits). To avoid further installs, the tutorial comes pre-packaged with a data file (of JSON records) and a set of images, which we're going to download into a temp directory now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_prefix = 'https://gitlab.com/pixeltable/python-sdk/-/raw/master/tutorials'\n",
    "json_data_url = f'{download_prefix}/coco-records.json'\n",
    "\n",
    "records = json.loads(urllib.request.urlopen(json_data_url).read().decode('utf-8'))\n",
    "\n",
    "image_dir = tempfile.mkdtemp()\n",
    "for r in tqdm.notebook.tqdm(records):\n",
    "    filename = r['filepath'].split('/')[1]\n",
    "    out_filepath = f'{image_dir}/{filename}'\n",
    "    url = f'{download_prefix}/{r[\"filepath\"]}'\n",
    "    r['filepath'] = out_filepath\n",
    "    img_data = urllib.request.urlopen(url).read()\n",
    "    with open(out_filepath, 'wb') as img_file:\n",
    "        img_file.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeae148",
   "metadata": {},
   "source": [
    "Each data record is a dictionary with top-level fields `filepath`, `tag`, `metadata`, and `ground_truth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191ff6b",
   "metadata": {},
   "source": [
    "## Creating a table\n",
    "\n",
    "A table for this data requires a column for each top-level field: `filepath`, `tag`, `metadata`, `ground_truth`.\n",
    "\n",
    "Instead of a file path per image we are going to store the image directly. The table columns are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f23b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    pt.Column('img', pt.ImageType(), nullable=False, indexed=True),\n",
    "    pt.Column('tag', pt.StringType(), nullable=False),\n",
    "    pt.Column('metadata', pt.JsonType(), nullable=False),\n",
    "    pt.Column('ground_truth', pt.JsonType(), nullable=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c633bdf",
   "metadata": {},
   "source": [
    "`nullable=False` means the values in this column can't be `None`, which Pixeltable will check at data insertion time. `indexed=True` tells Pixeltable to create a vector index for embeddings (using CLIP) for the images in this column, which enables text and image similarity search. More on that later.\n",
    "\n",
    "The available data types in Pixeltable are:\n",
    "\n",
    "|Pixeltable type|Python type|\n",
    "|:---|:---|\n",
    "| `pt.StringType()`| `str` |\n",
    "| `pt.IntType()`| `int` |\n",
    "| `pt.FloatType()`| `float` |\n",
    "| `pt.BoolType()`| `bool` |\n",
    "| `pt.TimestampType()`| `datetime.datetime` |\n",
    "| `pt.JsonType()`| lists and dicts that can be converted to JSON|\n",
    "| `pt.ArrayType()`| `numpy.ndarray`|\n",
    "| `pt.ImageType()`| `PIL.Image.Image`|\n",
    "| `pt.VideoType()`| `str` (the file path)|\n",
    "\n",
    "\n",
    "We then create a table `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d91e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = db.create_table('data', schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b8210",
   "metadata": {},
   "source": [
    "At this point, table `data` contains no data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ae861",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07636798",
   "metadata": {},
   "source": [
    "## Inserting data\n",
    "\n",
    "In order to populate `data` with what's in `records`, we turn the latter into a Pandas DataFrame and insert that with the `insert_pandas()` function (and we rename the `filepath` column to `img` to match the table definition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame.from_records(records).rename({'filepath': 'img'}, axis=1)\n",
    "data.insert_pandas(pd_df)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523e8cc",
   "metadata": {},
   "source": [
    "In Pixeltable, images are 'inserted' as file paths, and Pixeltable only stores these paths and not the images themselves, so there is no duplication of storage.\n",
    "\n",
    "Let's look at the first 3 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a119cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab49f2a",
   "metadata": {},
   "source": [
    "You can also insert the data directly, without prior conversion to a Pandas DataFrame, with the `insert_rows()` function, which requires a list of rows, each of which is a list of column values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    [r['filepath'], r['tag'], r['metadata'], r['ground_truth']] for r in records\n",
    "]\n",
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28143186",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert_rows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd07d3ea",
   "metadata": {},
   "source": [
    "We have now loaded our data twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3fe1bc",
   "metadata": {},
   "source": [
    "## Versioning in Pixeltable\n",
    "\n",
    "Pixeltable maintains a version history for data changes to tables (ie, inserting data and adding/dropping columns). The `revert()` method lets you go back to the preceding version.\n",
    "\n",
    "For our table `data`, since we don't want duplicates, we revert the last update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.revert()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5697416",
   "metadata": {},
   "source": [
    "## Data persistence\n",
    "\n",
    "Unlike \"computational containers\" such as Pandas or Dask DataFrames, tables in Pixeltable are persistent. To illustrate that, let's create a new Pixeltable client and a new handle to the `data` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc444e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pt.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33570d52",
   "metadata": {},
   "source": [
    "We already have a database `tutorials`, so now we call `get_db()` instead of `create_db()` (in fact, the latter would return with an exception). Likewise, we call `get_table()` to get a handle to the already present `data` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = cl.get_db('tutorial')\n",
    "data = db.get_table('data')\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5200f9c",
   "metadata": {},
   "source": [
    "# Retrieving data\n",
    "\n",
    "The Pixeltable retrieval interface is patterned after Pandas DataFrame operations: the index (`[]`) operator is used both to select columns for output and filter rows.\n",
    "\n",
    "## Filtering rows\n",
    "\n",
    "For example, to only look at test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b07f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.tag == 'test'].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07b8d3",
   "metadata": {},
   "source": [
    "Or at data for images that are less than 640 pixels wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.metadata.width < 640].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f36be1",
   "metadata": {},
   "source": [
    "Pixeltable supports the standard comparison operators (`<`, `<=`, `>`, `>=`, `==`) and logical operators (`&` for `and`, `|` for `or`, `~` for `not`). Like in Pandas, logical operators need to be wrapped in parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data.tag == 'test') & (data.metadata.width < 640)].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432e16d",
   "metadata": {},
   "source": [
    "## Selecting output\n",
    "\n",
    "Let's retrieve columns `tag` and `metadata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09185f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.tag, data.metadata].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc0569",
   "metadata": {},
   "source": [
    "In general, each element in `[]` needs to be a Pixeltable **expression**. In the previous example, the expressions were simple column references, but Pixeltable also supports most standard arithmetic operators as well as a set of type-specific functions (more on those in a bit). For example, to retrieve the total number of pixels per image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88984ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.tag, data.metadata.width * data.metadata.height].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482901f",
   "metadata": {},
   "source": [
    "### Operations on JSON data\n",
    "\n",
    "The previous example illustrates the use of path expressions against JSON-typed data: `width` is a field in the `metadata` column, which we can simply access as `data.metadata.width`.\n",
    "\n",
    "Another example: retrieve only the bounding boxes from the `ground_truth` column. This will come in handy later when we need to pass those bounding boxes (and not the surrounding dictionary) into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d12fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.ground_truth.detections['*'].bounding_box].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818d529",
   "metadata": {},
   "source": [
    "The field `detections` contains a list, and the `'*'` index indicates that you want all elements in that list. You can also use standard Python list indexing and slicing operations, such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.ground_truth.detections[0].bounding_box].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee6d43",
   "metadata": {},
   "source": [
    "to select only the first bounding box, or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.ground_truth.detections[::-1].bounding_box].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6bbec",
   "metadata": {},
   "source": [
    "to select the bounding boxes in reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29946736",
   "metadata": {},
   "source": [
    "### Operations on image data\n",
    "\n",
    "Image data has properties `width`, `height`, and `mode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.img.width, data.img.height, data.img.mode].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26433a9e",
   "metadata": {},
   "source": [
    "Pixeltable also has a number of built-in functions for images (these are a subset of what is available for `PIL.Image.Image`): \n",
    "\n",
    "|Image function||\n",
    "|:---|:---|\n",
    "|[`convert()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert)|Returns a converted copy of this image|\n",
    "|[`crop()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.crop)|Returns a rectangular region from this image|\n",
    "|[`effect_spread()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.effect_spread)|Randomly spread pixels in an image|\n",
    "|[`entropy()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.entropy)|Calculates and returns the entropy for the image|\n",
    "|[`filter()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.filter)|Filters this image using the given filter|\n",
    "|[`getbands()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getbands)|Returns a tuple containing the name of each band in this image|\n",
    "|[`getbbox()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getbbox)|Calculates the bounding box of the non-zero regions in the image|\n",
    "|[`getchannel()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getchannel)|Returns an image containing a single channel of the source image|\n",
    "|[`getcolors()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getcolors)|Returns a list of colors used in this image|\n",
    "|[`getextrema()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getextrema)|Gets the minimum and maximum pixel values for each band in the image|\n",
    "|[`getpalette()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getpalette)|Returns the image palette as a list|\n",
    "|[`getpixel()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getpixel)|Returns the pixel value at a given position|\n",
    "|[`getprojection()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getprojection)|Get projection to x and y axes|\n",
    "|[`histogram()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.histogram)|Returns a histogram for the image|\n",
    "|[`point()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.point)|Maps this image through a lookup table or function|\n",
    "|[`quantize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.quantize)|Convert the image to ‘P’ mode with the specified number of colors|\n",
    "|[`reduce()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.reduce)|Returns a copy of the image reduced factor times|\n",
    "|[`remap_palette()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.remap_palette)|Rewrites the image to reorder the palette|\n",
    "|[`resize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.resize)|Returns a resized copy of this image|\n",
    "|[`rotate()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.rotate)|Returns a rotated copy of this image|\n",
    "|[`transform()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.transform)|Transforms this image|\n",
    "|[`transpose()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.transpose)|Transpose image (flip or rotate in 90 degree steps)|\n",
    "\n",
    "These functions are invoked in the style of method calls and can be chained, as in this example, which rotates the image by 30 degrees and converts it to BW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.img.rotate(30).convert('L')].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c628c2c",
   "metadata": {},
   "source": [
    "### Image similarity search\n",
    "\n",
    "When we created the `frame` column we specified `indexed=True`, which creates a vector index of CLIP embeddings for the images in that column. We can take advantage of that with the search functions `nearest()` and `matches()`. First, let's get a sample image from `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = data[data.img].show(1)[0, 0]\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1294840",
   "metadata": {},
   "source": [
    "`show()` returns a result set, which is a two-dimensional structure you can access with standard Python indexing operations (ie, `[<row-idx>, <column-idx>]`. In this case, we're selecting the first column value of the first row, which is a `PIL.Image.Image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a51c8",
   "metadata": {},
   "source": [
    "To look for images like this one, use `nearest()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d14a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[data.img.nearest(sample_img)][data.img].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64048c88",
   "metadata": {},
   "source": [
    "To look for images based on text, use `matches()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3bdfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[data.img.matches('car')][data.img].show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306a9aa",
   "metadata": {},
   "source": [
    "# User-defined functions\n",
    "\n",
    "User-defined functions let you customize Pixeltable's functionality for your own data.\n",
    "\n",
    "In this example, we're going use a `torchvision` object detection model (Faster R-CNN) against the images in `data` with a user-defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=\"DEFAULT\")\n",
    "model.eval()  # switch to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547039ca",
   "metadata": {},
   "source": [
    "Our function converts the image to PyTorch format and obtains a prediction from the model, which is a list of dictionaries with fields `boxes`, `labels`, and `scores` (one per input image). The fields themselves are PyTorch tensors, and we convert them to standard Python lists (so they become JSON-serializable data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasterrcnn_detect(img):\n",
    "    t = transforms.ToTensor()(img)\n",
    "    t = transforms.ConvertImageDtype(torch.float)(t)\n",
    "    result = model([t])[0]\n",
    "    return {\n",
    "        'boxes': result['boxes'].tolist(), 'labels': result['labels'].tolist(), 'scores': result['scores'].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d45de",
   "metadata": {},
   "source": [
    "Let's confirm that `fasterrcnn_detect()` works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69143bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = fasterrcnn_detect(sample_img)\n",
    "sample_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3111ab",
   "metadata": {},
   "source": [
    "We now need to create a wrapper to `fasterrcnn_detect()` in order to tell Pixeltable what arguments the function takes and what it returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect = pt.make_function(pt.JsonType(), [pt.ImageType()], fasterrcnn_detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79b7a9",
   "metadata": {},
   "source": [
    "The first `make_function()` parameter is the return type, the second parameter is the list of parameter types, and the last parameter is the actual function we want Pixeltable to call.\n",
    "\n",
    "We can then use `detect` in the Pixeltable index operator using standard Python function call syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.img, detect(data.img)].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aa6fc",
   "metadata": {},
   "source": [
    "`detect` returns JSON data, and we can use Pixeltable's JSON functionality to access that as well. For example, if we're only interested in the first detected bounding box and the first label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[detect(data.img).boxes[0], detect(data.img).labels[0]].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c335f9",
   "metadata": {},
   "source": [
    "When running this query, Pixeltable evalutes `detect(data.img)` only once per row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32812769",
   "metadata": {},
   "source": [
    "## Stored functions\n",
    "\n",
    "Functions, like tables, can be stored in the database, which assigns them a name and makes them persistent (via pickling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.create_function('frcnn_detect', detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ded347",
   "metadata": {},
   "source": [
    "Just like a table, you can now get a handle to the function and use it without having access to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_udf = db.get_function('frcnn_detect')\n",
    "data[data.img, detect_udf(data.img).boxes[0]].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8104e",
   "metadata": {},
   "source": [
    "We can also check that it's still the same function with `list()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_udf.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd5141",
   "metadata": {},
   "source": [
    "We can see what functions are available across all databases with `list_functions()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.list_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e8e31",
   "metadata": {},
   "source": [
    "## Computed columns\n",
    "\n",
    "Being able to run models against any image stored in Pixeltable is very useful, but the runtime cost of model inference makes it impractical to run it every time we want to do something with the model output. In Pixeltable, we can use computed columns to precompute and cache the output of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_column(pt.Column('detections', computed_with=detect(data.img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d862524",
   "metadata": {},
   "source": [
    "`detections` is now a column in `data` which holds the model prediction for the `img` column. Like any other column, it is persistent. Pixeltables runs the computation  automatically whenever new data is added to the table. Let's see what `data` looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0ba8e",
   "metadata": {},
   "source": [
    "In general, the `computed_with` keyword argument can be any Pixeltable expression. In this example, we're making the first label recorded in `detections` available as a separate column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_column(pt.Column('first_label', computed_with=data.detections.labels[0]))\n",
    "data[data.detections.labels, data.first_label].show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
